This project explores a machine learning approach to detect hallucinations in responses generated by large language models. Textual question–answer pairs are converted into numerical features using cosine similarity between MiniLM sentence embeddings. A Gradient Boosting classifier is trained on these features to distinguish hallucinated and non-hallucinated responses. The project includes careful train–test splitting to avoid data leakage, ablation studies to evaluate feature impact, and discussion of ethical concerns such as reliability and bias in LLM outputs.
Implemented fully in Python using open-source tools.

## Data Format

The dataset is provided in **JSON / Arrow** format through the **HuggingFace datasets library** and loaded directly into Python.

Each data sample contains:

- **Question**: The original user query or prompt  
- **Reference Answer**: A ground-truth or trusted answer  
- **LLM Response**: The answer generated by a large language model  
- **Label**: Binary indicator of hallucination  
  - `1` → Hallucinated response  
  - `0` → Non-hallucinated response  

---

## Inputs and Outputs

### Input

- Textual **question–answer pairs**
- **LLM-generated responses** and corresponding **reference answers**

---

## Feature Engineering

- Sentences are converted into dense vector representations using **MiniLM sentence embeddings**
- **Cosine similarity** is computed between:
  - Question and LLM response  
  - Reference answer and LLM response  
- Additional lightweight textual features (e.g. **length ratios**) are used

---

### Output

- **Binary classification** indicating whether an LLM response is hallucinated or not : Factual / Hallucinated

---

## Machine Learning Task

- **Problem Type**: Supervised binary classification  
- **Model Used**: Gradient Boosting Classifier  
- **Evaluation**: ROC-AUC and qualitative error analysis  
- **Validation**: Careful train–test splitting to avoid question leakage  
- **Ablation Study**: Conducted to measure the impact of individual features on model performance  

---

## Ethical Considerations

This project discusses ethical risks related to:

- Reliability of LLM outputs  
- False confidence in hallucinated answers  
- Bias inherited from training data  
- Limitations of automated hallucination detection systems




## **MedHallu Dataset**

MedHallu Contributors. (2023). *MedHallu: A dataset for hallucination detection in large language model responses*. HuggingFace Datasets.  
https://huggingface.co/datasets/medhallu

