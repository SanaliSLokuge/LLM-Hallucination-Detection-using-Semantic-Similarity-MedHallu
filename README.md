This project explores a machine learning approach to detect hallucinations in responses generated by large language models. Textual question–answer pairs are converted into numerical features using cosine similarity between MiniLM sentence embeddings. A Gradient Boosting classifier is trained on these features to distinguish hallucinated and non-hallucinated responses. The project includes careful train–test splitting to avoid data leakage, ablation studies to evaluate feature impact, and discussion of ethical concerns such as reliability and bias in LLM outputs.
Implemented fully in Python using open-source tools.
